{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim: Predict if PM concentration is higher than boundary value based on meteorologic parameters, leave out influence of traffic data.\n",
    "daily boundary value for PM10:  50 µg/m³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f0ea06ef23f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Load PM dataset (target var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            P1\n",
      "timestamp                     \n",
      "1970-01-01 00:00:00  61.175455\n",
      "1970-01-01 01:00:00  60.337778\n",
      "1970-01-01 02:00:00  60.901000\n",
      "1970-01-01 03:00:00  58.587727\n",
      "1970-01-01 04:00:00  53.806000\n",
      "1970-01-01 05:00:00  42.638333\n",
      "1970-01-01 06:00:00  42.976667\n",
      "1970-01-01 07:00:00  45.372917\n",
      "1970-01-01 08:00:00  43.117500\n",
      "1970-01-01 09:00:00  46.309130\n",
      "1970-01-01 10:00:00  38.529583\n",
      "1970-01-01 11:00:00  33.917500\n",
      "1970-01-01 12:00:00  31.267083\n",
      "1970-01-01 13:00:00  28.494000\n",
      "1970-01-01 14:00:00  26.772917\n",
      "1970-01-01 15:00:00  25.041667\n",
      "1970-01-01 16:00:00  23.417917\n",
      "1970-01-01 17:00:00  32.340417\n",
      "1970-01-01 18:00:00  42.986818\n",
      "1970-01-01 19:00:00  40.723913\n",
      "1970-01-01 20:00:00  43.877083\n",
      "1970-01-01 21:00:00  47.411250\n",
      "1970-01-01 22:00:00  48.723333\n",
      "1970-01-01 23:00:00  38.984737\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r\"C:\\Users\\Anna\\Documents\\UNI\\MA Semi 1\\Machine Learning\\Final_project\")\n",
    "\n",
    "## test file from luftdateninfo\n",
    "file_pm = \"test_PM_2020-01-23_sds011_sensor_10963.csv\"\n",
    "pm_ds = pd.read_csv(file_pm, sep=\";\", index_col=\"timestamp\")\n",
    "\n",
    "## df with time_col as index\n",
    "pm_ds = pm_ds[[\"P1\"]]\n",
    "\n",
    "#Sum up to hourly values\n",
    "pm_ds.index = pd.to_datetime(pm_ds.index) # convert index into time_dtype\n",
    "\n",
    "pm_ds_hour = pm_ds.groupby(pm_ds.index.hour).mean()# ds= one day -->24 rows\n",
    "pm_ds_hour.index = pd.to_datetime(pm_ds_hour.index, unit=\"h\")\n",
    "print(pm_ds_hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load meteorologic test-file and merge both ds (PM + meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              F    D   R1     P0  TT_TU  RF_TU\n",
      "MESS_DATUM                                    \n",
      "1995100414  2.6  220  0.0  970.6   23.3   51.0\n",
      "1995100415  3.6  210  0.0  970.3   23.8   50.0\n",
      "1995100416  1.5  150  0.0  969.9   23.0   54.0\n",
      "1995100417  1.5  120  0.0  970.0   21.5   63.0\n",
      "1995100418  1.5  180  0.0  970.1   19.4   71.0\n",
      "...         ...  ...  ...    ...    ...    ...\n",
      "2019123119  1.3  270  0.0  987.3    0.3   82.0\n",
      "2019123120  0.9  270  0.0  987.8    0.9   84.0\n",
      "2019123121  1.0  230  0.0  987.2    1.2   87.0\n",
      "2019123122  0.9  300  0.0  987.5    1.4   87.0\n",
      "2019123123  0.5  230  0.0  987.3    1.1   89.0\n",
      "\n",
      "[211931 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "file_wind =  pd.read_csv(\"./DWDstats_meteo/produkt_ff_stunde_19530101_20191231_04931.txt\", sep=\";\", header=0,\n",
    "                         index_col=\"MESS_DATUM\")\n",
    "#print(file_wind)\n",
    "file_rain = pd.read_csv(\"./DWDstats_meteo/produkt_rr_stunde_19951004_20191231_04931.txt\", sep=\";\", header=0,\n",
    "                         index_col=\"MESS_DATUM\")\n",
    "file_pressure = pd.read_csv(\"./DWDstats_meteo/produkt_p0_stunde_19490101_20191231_04931.txt\", sep=\";\", header=0,\n",
    "                          index_col=\"MESS_DATUM\") \n",
    "file_temperature = pd.read_csv(\"./DWDstats_meteo/produkt_tu_stunde_19880101_20191231_04931.txt\", sep=\";\", header=0,\n",
    "                          index_col=\"MESS_DATUM\") \n",
    "\n",
    "allmeteo = file_wind.merge(file_rain, on =\"MESS_DATUM\").merge(file_pressure, on =\"MESS_DATUM\").merge(file_temperature, on =\"MESS_DATUM\")\n",
    "allmeteo = allmeteo[[\"   F\", \"   D\", \"  R1\", \"  P0\", \"TT_TU\", \"RF_TU\"]]\n",
    "allmeteo = allmeteo.rename(columns={\"   F\": \"F\", \"   D\":\"D\", \"  R1\": \"R1\", \"  P0\": \"P0\"})\n",
    "\n",
    "print(allmeteo)\n",
    "\n",
    "\n",
    "####################\n",
    "#import glob\n",
    "#files_meteo = glob.glob('./DWDstats_meteo/*04931.txt')\n",
    "\n",
    "# allmeteo_ds = \n",
    "# for file in files_meteo:\n",
    "#     meteo_ds_hour = pd.read_csv(file, sep=\";\", index_col=\"MESS_DATUM\")\n",
    "#     print(meteo_ds_hour)\n",
    "    \n",
    "#df1.merge(df2, on =\"MESS_DATUM\")\n",
    "####################################\n",
    "\n",
    "# ## df with time_col as index\n",
    "# pm_ds = pm_ds[[\"P1\"]]\n",
    "\n",
    "# #Sum up to hourly values\n",
    "# pm_ds.index = pd.to_datetime(pm_ds.index) # convert index into time_dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['1970-01-01 00:00:01.995100414',\n",
      "               '1970-01-01 00:00:01.995100415',\n",
      "               '1970-01-01 00:00:01.995100416',\n",
      "               '1970-01-01 00:00:01.995100417',\n",
      "               '1970-01-01 00:00:01.995100418',\n",
      "               '1970-01-01 00:00:01.995100419',\n",
      "               '1970-01-01 00:00:01.995100420',\n",
      "               '1970-01-01 00:00:01.995100421',\n",
      "               '1970-01-01 00:00:01.995100422',\n",
      "               '1970-01-01 00:00:01.995100423',\n",
      "               ...\n",
      "               '1970-01-01 00:00:02.019123114',\n",
      "               '1970-01-01 00:00:02.019123115',\n",
      "               '1970-01-01 00:00:02.019123116',\n",
      "               '1970-01-01 00:00:02.019123117',\n",
      "               '1970-01-01 00:00:02.019123118',\n",
      "               '1970-01-01 00:00:02.019123119',\n",
      "               '1970-01-01 00:00:02.019123120',\n",
      "               '1970-01-01 00:00:02.019123121',\n",
      "               '1970-01-01 00:00:02.019123122',\n",
      "               '1970-01-01 00:00:02.019123123'],\n",
      "              dtype='datetime64[ns]', name='MESS_DATUM', length=211931, freq=None)\n"
     ]
    }
   ],
   "source": [
    "allmeteo2 = allmeteo\n",
    "## convert both indeces to same datetime\n",
    "## style: YYYY-MM-DDTHH:MM:SS2020-01-23T23:47:57\n",
    "print\n",
    "allmeteo2.index = pd.to_datetime(allmeteo2.index,format=\"%Y%m%d%H\", unit=\"ns\")\n",
    "#\n",
    "#allmeteo.index = pd.to_datetime(allmeteo.index, unit=\"h\")\n",
    "print(allmeteo2.index)\n",
    "\n",
    "\n",
    "# ### merge both DS (meteo Ds + PM DS)\n",
    "# print(type(allmeteo.index))\n",
    "# PM_meteo = pm_ds_hour.merge(allmeteo, left_index = \"timestamp\", right_index=\"MESS_DATUM\")\n",
    "# PM_meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Split Dataset in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the boston hoursing dataset into training and testing \n",
    "\n",
    "ratio = 0.2 # split ratio\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=ratio,random_state=5)\n",
    "\n",
    "#print the size of train and test dataset\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Use Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softReg = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')\n",
    "softReg.fit(Xstan,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softReg.intercept_,softReg.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = softReg.predict(Xstan)\n",
    "dataStan['predict'] = yhat\n",
    "dataStan.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of Softmax permorfance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = confusion_matrix(dataStan['category'].values,yhat)\n",
    "confusionMatrix = pd.DataFrame(data = C, index=['poor(0), true','good(1), true','great(2), true'], columns = ['poor(0), predicted','good(1), predicted','great(2), predicted'])\n",
    "confusionMatrix.loc['sum'] = confusionMatrix.sum()\n",
    "confusionMatrix['sum'] = confusionMatrix.sum(axis=1)\n",
    "confusionMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
